{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ray\n",
    "import numpy as np\n",
    "import xgboost_ray as xgbr\n",
    "import xgboost as xgb\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from xgboost_ray.tune import TuneReportCheckpointCallback\n",
    "SIMPLE = True\n",
    "RAY_PARAMS = xgbr.RayParams(max_actor_restarts=1, num_actors=1, cpus_per_actor=1)\n",
    "ROOT_DIR = f'{os.getcwd()}/..'\n",
    "LOCAL_DIR = f'{ROOT_DIR}/ray_results'\n",
    "LOCAL_IP = '192.168.1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def load_dataframe() -> \"ray.ObjectRef\":\n",
    "    \"\"\"\n",
    "    build dataframe from breast cancer dataset\n",
    "    \"\"\"\n",
    "    print(\"Loading CSV.\")\n",
    "    if SIMPLE:\n",
    "        print(\"Loading simple\")\n",
    "        from sklearn import datasets\n",
    "        data = datasets.load_breast_cancer(return_X_y=True)\n",
    "    else:\n",
    "        import pandas as pd\n",
    "        # import modin.pandas as mpd -- currently does not work.\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \\\n",
    "         \"00280/HIGGS.csv.gz\"\n",
    "        colnames = [\"label\"] + [\"feature-%02d\" % i for i in range(1, 29)]\n",
    "        data = pd.read_csv(url, compression='gzip', names=colnames)\n",
    "        # data = pd.read_csv(\"/home/astro/HIGGS.csv.gz\", names=colnames)\n",
    "        print(\"loaded higgs\")\n",
    "    print(\"Loaded CSV.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray_task(**task_args)\n",
    "def split_train_test(data):\n",
    "    print(\"Splitting Data to Train and Test Sets\")\n",
    "    logfile = open(\"/tmp/ray/session_latest/custom.log\", \"w\")\n",
    "    def write(msg):\n",
    "        logfile.write(f\"{msg}\\n\")\n",
    "        logfile.flush()\n",
    "\n",
    "    print(f\"Creating data matrix: {data, SIMPLE}\")\n",
    "    if SIMPLE:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        print(\"Splitting data\")\n",
    "        data, labels = data\n",
    "        train_x, test_x, train_y, test_y = train_test_split(\n",
    "            data, labels, test_size=0.25)\n",
    "\n",
    "        train_set = xgbr.RayDMatrix(train_x, train_y)\n",
    "        test_set = xgbr.RayDMatrix(test_x, test_y)\n",
    "    else:\n",
    "        df_train = data[(data['feature-01'] < 0.4)]\n",
    "        colnames = [\"label\"] + [\"feature-%02d\" % i for i in range(1, 29)]\n",
    "        train_set = xgbr.RayDMatrix(df_train, label=\"label\", columns=colnames)\n",
    "        df_validation = data[(data['feature-01'] >= 0.4)& (data['feature-01'] < 0.8)]\n",
    "        test_set = xgbr.RayDMatrix(df_validation, label=\"label\")\n",
    "    print(\"finished data matrix\")\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    config,\n",
    "    checkpoint_dir=None,\n",
    "    data_dir=None,\n",
    "    data=()\n",
    "):\n",
    "    logfile = open(\"/tmp/ray/session_latest/custom.log\", \"w\")\n",
    "    def write(msg):\n",
    "        logfile.write(f\"{msg}\\n\")\n",
    "        logfile.flush()\n",
    "\n",
    "    dtrain, dvalidation = data\n",
    "    evallist = [(dvalidation, 'eval')]\n",
    "    # evals_result = {}\n",
    "    config = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "    print(\"Start training\")\n",
    "    bst = xgbr.train(\n",
    "        params=config,\n",
    "        dtrain=dtrain,\n",
    "        ray_params=RAY_PARAMS,\n",
    "        num_boost_round=100,\n",
    "        evals=evallist,\n",
    "        callbacks=[TuneReportCheckpointCallback(filename=f\"model.xgb\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def tune_model(data):\n",
    "    logfile = open(\"/tmp/ray/session_latest/custom.log\", \"w\")\n",
    "    def write(msg):\n",
    "        logfile.write(f\"{msg}\\n\")\n",
    "        logfile.flush()\n",
    "\n",
    "    search_space = {\n",
    "        # You can mix constants with search space objects.\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"max_depth\": tune.randint(1, 9),\n",
    "        \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    "    }\n",
    "\n",
    "    print(\"enabling aggressive early stopping of bad trials\")\n",
    "    # This will enable aggressive early stopping of bad trials.\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=4,  # 10 training iterations\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    print(\"Tuning\")\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_model,\n",
    "            data=data\n",
    "            # checkpoint_dir=f\"{LOCAL_DIR}/checkpoints\",\n",
    "            # data_dir=f\"{LOCAL_DIR}/data\",\n",
    "        ),\n",
    "        metric=\"eval-logloss\",\n",
    "        mode=\"min\",\n",
    "        local_dir=LOCAL_DIR,\n",
    "        # You can add \"gpu\": 0.1 to allocate GPUs\n",
    "        resources_per_trial=RAY_PARAMS.get_tune_resources(),\n",
    "        config=search_space,\n",
    "        num_samples=4,\n",
    "        scheduler=scheduler)\n",
    "\n",
    "    print(\"Done Tuning\")\n",
    "\n",
    "    return analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def load_best_model_checkpoint(analysis):\n",
    "    print(\"Running\")\n",
    "    logfile = open(\"/tmp/ray/session_latest/custom.log\", \"w\")\n",
    "    def write(msg):\n",
    "        logfile.write(f\"{msg}\\n\")\n",
    "        logfile.flush()\n",
    "\n",
    "    print(\"Checking Analysis\")\n",
    "    print(f\"Checkpoint is at: {analysis.best_checkpoint}\")\n",
    "\n",
    "    best_bst = xgb.Booster()\n",
    "    print(f\"Analysis Best Result on eval-error is: {analysis.best_result['eval-error']}\")\n",
    "    print(\"Loading Model with Best Params\")\n",
    "    best_bst.load_model(os.path.join(analysis.best_checkpoint, \"model.xgb\"))\n",
    "    accuracy = 1. - analysis.best_result[\"eval-error\"]        \n",
    "\n",
    "    print(f\"Best model parameters: {analysis.best_config}\")\n",
    "    print(f\"Best model total accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # We could now do further predictions with\n",
    "    # best_bst.predict(...)\n",
    "    return best_bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.util.connect(f'{LOCAL_IP}:10001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_raw_df = load_dataframe.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = split_train_test.remote(build_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune_model.remote(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = load_best_model_checkpoint.remote(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could take best_checkpoint to call predict from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
